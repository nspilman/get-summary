{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b03738-ffc7-40ea-a7da-5fcbd7b803bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in ./.venv/lib/python3.12/site-packages (2.11.1)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in ./.venv/lib/python3.12/site-packages (from scrapy) (24.3.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in ./.venv/lib/python3.12/site-packages (from scrapy) (42.0.5)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in ./.venv/lib/python3.12/site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in ./.venv/lib/python3.12/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in ./.venv/lib/python3.12/site-packages (from scrapy) (1.9.0)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in ./.venv/lib/python3.12/site-packages (from scrapy) (24.1.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in ./.venv/lib/python3.12/site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in ./.venv/lib/python3.12/site-packages (from scrapy) (24.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in ./.venv/lib/python3.12/site-packages (from scrapy) (2.1.2)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in ./.venv/lib/python3.12/site-packages (from scrapy) (6.2)\n",
      "Requirement already satisfied: protego>=0.1.15 in ./.venv/lib/python3.12/site-packages (from scrapy) (0.3.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in ./.venv/lib/python3.12/site-packages (from scrapy) (0.8.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from scrapy) (69.2.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from scrapy) (24.0)\n",
      "Requirement already satisfied: tldextract in ./.venv/lib/python3.12/site-packages (from scrapy) (5.1.2)\n",
      "Requirement already satisfied: lxml>=4.4.1 in ./.venv/lib/python3.12/site-packages (from scrapy) (5.1.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in ./.venv/lib/python3.12/site-packages (from scrapy) (2.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in ./.venv/lib/python3.12/site-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in ./.venv/lib/python3.12/site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in ./.venv/lib/python3.12/site-packages (from service-identity>=18.1.0->scrapy) (23.2.0)\n",
      "Requirement already satisfied: pyasn1 in ./.venv/lib/python3.12/site-packages (from service-identity>=18.1.0->scrapy) (0.5.1)\n",
      "Requirement already satisfied: pyasn1-modules in ./.venv/lib/python3.12/site-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
      "Requirement already satisfied: automat>=0.8.0 in ./.venv/lib/python3.12/site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: constantly>=15.1 in ./.venv/lib/python3.12/site-packages (from Twisted>=18.9.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in ./.venv/lib/python3.12/site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=22.10.0 in ./.venv/lib/python3.12/site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./.venv/lib/python3.12/site-packages (from Twisted>=18.9.0->scrapy) (4.10.0)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from tldextract->scrapy) (3.6)\n",
      "Requirement already satisfied: requests>=2.1.0 in ./.venv/lib/python3.12/site-packages (from tldextract->scrapy) (2.31.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in ./.venv/lib/python3.12/site-packages (from tldextract->scrapy) (2.0.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in ./.venv/lib/python3.12/site-packages (from tldextract->scrapy) (3.13.1)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.12/site-packages (from automat>=0.8.0->Twisted>=18.9.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.1.0->tldextract->scrapy) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f824f15f-0054-4d3d-988a-87a9d9a36b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natespilman/Repos/ScrapyPlayground/pioneer/pioneer/.venv/lib/python3.12/site-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloads/2024-04-05/portland-buy-local-shifts-to-free-business-directory-listings-new-strategic-focus.txt\n",
      "downloads/2024-04-05/how-to-avoid-having-a-board-thats-a-weak-link.txt\n",
      "downloads/2024-04-05/new-manager-of-a-downtown-freeport-shopping-center-looks-to-increase-visibility.txt\n",
      "downloads/2024-04-05/engineering-firm-haley-ward-makes-5-leadership-promotions.txt\n",
      "downloads/2024-04-05/vermont-construction-firm-bullish-on-southern-maine-market-opens-portland-office.txt\n",
      "downloads/2024-04-05/take-me-to-the-river-talking-head-david-byrne-talks-about-dam-removal-in-maine.txt\n",
      "downloads/2024-04-05/deer-isle-restaurant-ranks-no-2-among-best-in-us-says-food-wine-portland-is-a-top-10-city.txt\n",
      "downloads/2024-04-05/hallowell-area-board-of-trade-names-its-next-president.txt\n",
      "downloads/2024-04-05/maine-power-companies-are-gearing-up-for-another-winter-like-storm.txt\n",
      "downloads/2024-04-05/evergreen-credit-union-prepares-to-open-doors-at-its-new-scarborough-branch.txt\n",
      "downloads/2024-04-05/after-pandemic-delay-farmington-campus-opens-expanded-child-care-center.txt\n",
      "downloads/2024-04-05/maine-companies-with-global-reach-honored-by-mitc.txt\n",
      "downloads/2024-04-05/heres-what-you-need-to-earn-in-order-to-buy-a-home-in-maine-today.txt\n",
      "downloads/2024-04-05/4-foodie-finalists-from-maine-among-those-vying-for-prestigious-james-beard-awards.txt\n",
      "downloads/2024-04-05/with-a-new-board-and-new-ambitions-startup-maine-gears-up-for-may-conference.txt\n",
      "downloads/2024-04-05/friday-food-insider-after-10-years-limerick-brewery-calls-it-quits.txt\n",
      "downloads/2024-04-05/at-brunswick-landing-manufacturer-starc-plans-to-build-new-plant.txt\n",
      "downloads/2024-04-05/spring-storm-brings-snow-wind-rain-and-more-power-outages.txt\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.http import HtmlResponse\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "import logging\n",
    "\n",
    "import os\n",
    "from scrapy.dupefilters import BaseDupeFilter\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "class CustomDupeFilter(BaseDupeFilter):\n",
    "    def __init__(self, path='downloads'):\n",
    "        self.path = path\n",
    "        self.date_string = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "        self.filenames_seen = set()\n",
    "\n",
    "    def open(self):\n",
    "        if os.path.exists(self.path):\n",
    "            for directory in os.listdir(self.path):\n",
    "                for filename in os.listdir(os.path.join(self.path, directory)):\n",
    "                    self.filenames_seen.add(filename)\n",
    "            for filename in os.listdir(self.path):\n",
    "                self.filenames_seen.add(filename)\n",
    "\n",
    "    def close(self, reason):\n",
    "        pass\n",
    "\n",
    "    def request_seen(self, request):\n",
    "        filename = request.url.split('/')[-1] + '.txt'\n",
    "        if filename in self.filenames_seen:\n",
    "            return True\n",
    "        self.filenames_seen.add(filename)\n",
    "        return False\n",
    "\n",
    "class MainebizItem(scrapy.Item):\n",
    "    filename = scrapy.Field()\n",
    "    text_content = scrapy.Field()\n",
    "\n",
    "import os\n",
    "\n",
    "class MainebizPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        logging.basicConfig(level=logging.INFO, format='PROCESSING!!!!!')\n",
    "\n",
    "        # Get the filename and text content from the item\n",
    "        filename = item['filename']\n",
    "        text_content = item['text_content']\n",
    "        \n",
    "        # Create the downloads directory if it doesn't exist\n",
    "        import datetime\n",
    "        now = datetime.datetime.now()\n",
    "        date_string = now.strftime(\"%Y-%m-%d\")\n",
    "        download_dir = os.path.join('downloads', date_string)\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "        print(filepath)\n",
    "        \n",
    "        # Save the text content to a file\n",
    "        with open(filepath, 'w', encoding='utf-8') as file:\n",
    "            file.write(text_content)\n",
    "        \n",
    "        logging.info(f\"Saved file: {filename}\")\n",
    "        return item\n",
    "\n",
    "class MainebizSpider(scrapy.Spider):\n",
    "    name = 'mainebiz'\n",
    "    allowed_domains = ['mainebiz.biz']\n",
    "    start_urls = ['https://www.mainebiz.biz/']\n",
    "\n",
    "    custom_settings = {\n",
    "        'ITEM_PIPELINES': {\n",
    "            MainebizPipeline: 300\n",
    "        },\n",
    "        'DUPEFILTER_CLASS': CustomDupeFilter,\n",
    "    }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Extract all the links from the homepage\n",
    "        links = response.css('a::attr(href)').getall()\n",
    "        \n",
    "        logging.info(f\"Found {len(links)} links on the homepage\")\n",
    "        \n",
    "        # Follow each link and parse the text content\n",
    "        for link in links:\n",
    "            if link.startswith('/'):\n",
    "                # Construct the absolute URL\n",
    "                url = response.urljoin(link)\n",
    "                yield scrapy.Request(url, callback=self.parse_page)\n",
    "    \n",
    "    def parse_page(self, response):\n",
    "        # Create an HtmlResponse object from the response body\n",
    "        html_response = HtmlResponse(url=response.url, body=response.body, encoding='utf-8')\n",
    "        \n",
    "        # Extract the text content using XPath\n",
    "        text_content = ' '.join(html_response.xpath('//body//text()').getall())\n",
    "        \n",
    "        # Remove excessive whitespace and newline characters\n",
    "        text_content = ' '.join(text_content.split())\n",
    "        \n",
    "        # Generate a unique filename for each page\n",
    "        filename = response.url.split('/')[-1] + '.txt'\n",
    "        \n",
    "        # Create a MainebizItem and populate its fields\n",
    "        item = MainebizItem()\n",
    "        item['filename'] = filename\n",
    "        item['text_content'] = text_content\n",
    "        \n",
    "        logging.info(f\"Parsed page: {response.url}\")\n",
    "        yield item\n",
    "\n",
    "# Configure logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "runner = CrawlerRunner(settings={\n",
    "    'ITEM_PIPELINES': {\n",
    "        'MainebizPipeline': 300\n",
    "    }\n",
    "})\n",
    "\n",
    "d = runner.crawl(MainebizSpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
